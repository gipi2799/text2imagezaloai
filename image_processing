RESOLUTION = 256
    AUTO = tf.data.AUTOTUNE
    POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)

    augmenter = keras.Sequential(
        layers=[
            keras.layers.experimental.preprocessing.CenterCrop(RESOLUTION, RESOLUTION),
            keras_cv.layers.RandomFlip(),
            tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),
        ]
    )
    text_encoder = TextEncoder(MAX_PROMPT_LENGTH)

    def process_image(image_path, tokenized_text):
        image = tf.io.read_file(image_path)
        image = tf.io.decode_png(image, 3)
        image = tf.image.resize(image, (RESOLUTION, RESOLUTION))
        return image, tokenized_text

    def apply_augmentation(image_batch, token_batch):
        return augmenter(image_batch), token_batch

    def run_text_encoder(image_batch, token_batch):
        return (
            image_batch,
            token_batch,
            text_encoder([token_batch, POS_IDS], training=False),
        )

    def prepare_dict(image_batch, token_batch, encoded_text_batch):
        return {
            "images": image_batch,
            "tokens": token_batch,
            "encoded_text": encoded_text_batch,
        }

    def prepare_dataset(image_paths, tokenized_texts, batch_size=1):
        dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts))
        dataset = dataset.shuffle(batch_size * 10)
        dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)
        dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)
        dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)
        dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)
        return dataset.prefetch(AUTO)

    # Prepare the dataset.
    training_dataset = prepare_dataset(
        np.array(data_frame["bannerImage"]), tokenized_texts, batch_size=4
    )
    # Take a sample batch and investigate.
    sample_batch = next(iter(training_dataset))

    for k in sample_batch:
        print(k, sample_batch[k].shape)

print(training_dataset)
